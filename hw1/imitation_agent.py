#!/usr/bin/env python

"""
Code to create Behavioral cloning agent and DAgger agent to learn from data
generated by expert policy.

Author:Xi Liu
"""

import tensorflow as tf
import pickle
import numpy as np
import tf_util
import argparse
import load_policy
import gym
import os
import matplotlib.pyplot as plt

class BCAgent:
    def __init__(self, env_name, sess, batch_size=16, rollout_num=50,
                 hidden_layers=(64,64,64), epoch_num=100, test_step=2,
                 rollout_steps=1000, learning_rate=0.001):
        self.env_name = env_name
        self.var_scope_name = 'BC_' + env_name
        self.env = gym.make(env_name)
        self.sess = sess
        self.rollout_num = rollout_num
        self.epoch_num = epoch_num
        self.hid_1, self.hid_2, self.hid_3 = hidden_layers
        self.learning_rate = learning_rate
        self.states, self.actions = self.data_loader()
        self.batch_size = batch_size
        self.test_step = test_step
        self.rollout_steps = min(rollout_steps, self.env.spec.timestep_limit)
        self.test_epoch = []
        self.test_mean_rwd, self.test_std_rwd = [], []
        self.batch_num = int(len(self.states)/batch_size)
        with tf.variable_scope(self.var_scope_name):
            self.build_model()

    def data_loader(self):
        filename = 'expert_data/' + self.env_name + '.pkl'
        with open(filename, 'rb') as fr:
            data = pickle.loads(fr.read())
            states, actions = data['observations'], data['actions']
        return states, actions

    def next_batch(self):
        idxes = np.random.randint(self.states.shape[0], size=self.batch_size)
        s_batch = self.states[idxes]
        a_batch = self.actions[idxes]
        a_batch = a_batch.reshape(a_batch.shape[0], a_batch.shape[2])
        return s_batch, a_batch

    def build_model(self):
        n_states, n_actions = self.states.shape[-1], self.actions.shape[-1]
        self.states_ph = tf.placeholder(tf.float32, shape=[None, n_states], name='states_ph')
        self.actions_ph = tf.placeholder(tf.float32, shape=[None, n_actions], name='actions_ph')
        self.layer_1 = tf.layers.dense(self.states_ph, self.hid_1, activation=tf.nn.relu, name='layer_1')
        self.layer_2 = tf.layers.dense(self.layer_1, self.hid_2, activation=tf.nn.relu, name='layer_2')
        self.layer_3 = tf.layers.dense(self.layer_2, self.hid_3, activation=tf.nn.relu, name='layer_3')
        self.logits = tf.layers.dense(self.layer_2, n_actions, name='logits')
        self.loss = tf.losses.mean_squared_error(self.logits, self.actions_ph)
        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)
        tf.global_variables_initializer().run()

    def train_agent(self):
        for epoch in range(self.epoch_num):
            for batch in range(self.batch_num):
                s_batch, a_batch = self.next_batch()
                _, loss_ = self.sess.run([self.optimizer, self.loss],
                                         feed_dict={self.states_ph: s_batch,
                                               self.actions_ph: a_batch})

            if epoch % self.test_step == 0:
                print("current epoch is: ", epoch, " and loss is: ", loss_)
                _, mean_rwd, std_rwd = self.rollout()
                self.test_epoch.append(epoch)
                self.test_mean_rwd.append(mean_rwd)
                self.test_std_rwd.append(std_rwd)

    def rollout(self):
        rewards = []
        for _ in range(self.rollout_num):
            total_r = 0
            s = self.env.reset()
            for step in range(self.rollout_steps):
                a = self.sess.run(self.logits, feed_dict={self.states_ph: s[None, :]})
                next_s, r, done, _ = self.env.step(a)
                total_r += r
                if done:
                    break
                else:
                    s = next_s
            rewards.append(total_r)
        return rewards, np.mean(rewards), np.std(rewards)

class DAAgent(BCAgent):
    def __init__(self, env_name, sess, batch_size=16, rollout_num=50,
                 hidden_layers=(64,64,64), epoch_num=100, test_step=2,
                 rollout_steps=1000, learning_rate=0.001, data_collect_num=20,
                 data_agg_per_epoch = 2):
        self.env_name = env_name
        self.var_scope_name = 'DA_' + env_name
        self.env = gym.make(env_name)
        self.sess = sess
        self.rollout_num = rollout_num
        self.epoch_num = epoch_num
        self.hid_1, self.hid_2, self.hid_3 = hidden_layers
        self.learning_rate = learning_rate
        self.states, self.actions = self.data_loader()
        self.batch_size = batch_size
        self.test_step = test_step
        self.rollout_steps = min(rollout_steps, self.env.spec.timestep_limit)
        self.test_epoch = []
        self.test_mean_rwd, self.test_std_rwd = [], []
        self.batch_num = int(len(self.states)/batch_size)
        with tf.variable_scope(self.var_scope_name):
            self.build_model()

        self.data_collect_num = data_collect_num
        self.data_agg_per_epoch = data_agg_per_epoch
        self.policy_fn = load_policy.load_policy('experts/' + self.env_name + '.pkl')

    def train_agent(self):
        for epoch in range(self.epoch_num):
            for batch in range(self.batch_num):
                s_batch, a_batch = self.next_batch()
                _, loss_ = self.sess.run([self.optimizer, self.loss], feed_dict={self.states_ph: s_batch,
                                               self.actions_ph: a_batch})

            if epoch % self.test_step == 0:
                print("current epoch is: ", epoch, " and loss is: ", loss_)
                _, mean_rwd, std_rwd = self.rollout()
                self.test_epoch.append(epoch)
                self.test_mean_rwd.append(mean_rwd)
                self.test_std_rwd.append(std_rwd)

            if epoch % self.data_agg_per_epoch == 0:
                self.generate_data()

    def generate_data(self):
        """
        Generate new states based on current policy and correponding actions
        based on expert policy. The new data are added into training data.
        """
        new_states, new_actions = [], []
        for _ in range(self.data_collect_num):
            s = self.env.reset()
            for step in range(self.rollout_steps):
                exp_a = self.policy_fn(s[None,:])
                pred_a = self.sess.run(self.logits, feed_dict={self.states_ph: s[None, :]})
                new_states.append(s)
                new_actions.append(exp_a)
                next_s, r, done, _ = self.env.step(pred_a)
                if done:
                    break
                else:
                    s = next_s
        self.states = np.vstack((self.states, np.array(new_states)))
        self.actions = np.vstack((self.actions, np.array(new_actions)))

def load_expert_stats(env_name):
    filename = 'expert_data/' + env_name + '_stats.pkl'
    with open(filename, 'rb') as fr:
        data = pickle.loads(fr.read())
        mean_rwd, std_rwd = data['mean_return'], data['std_return']
    return mean_rwd, std_rwd

def save_stats_plot(env_name, bc_model, da_model):
    """
    Plot the mean rewards and std of rewards for behavioral cloning model and
    DAgger model. The mean rewards from the expert policy is the baseline.
    """
    expert_mean, expert_std = load_expert_stats(env_name)

    plt.axhline(expert_mean, color='k')
    plt.axhline(expert_mean + expert_std, linestyle = 'dashed', color='k', alpha=0.5)
    plt.axhline(expert_mean - expert_std, linestyle = 'dashed', color='k',alpha=0.5)
    plt.text(1, expert_mean, 'Baseline')

    plt.errorbar(bc_model.test_epoch, bc_model.test_mean_rwd,
                 yerr=bc_model.test_std_rwd, label="BC",
                 fmt='o', markersize=5, capsize=3, elinewidth=1)
    plt.errorbar(da_model.test_epoch, da_model.test_mean_rwd,
                 yerr=da_model.test_std_rwd, label="DAgger",
                 fmt='o', markersize=5, capsize=3, elinewidth=1)

    plt.grid()
    plt.legend()
    plt.title(env_name)
    plt.savefig('./imitation_data/stats_plot_' + env_name + '.png')

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('env_name', type=str)
    parser.add_argument("--learning_rate", type=float, default=0.001)
    parser.add_argument("--num_epochs", type=int, default=20)
    parser.add_argument("--max_timesteps", type=int, default=1000)
    parser.add_argument('--num_rollouts', type=int, default=20,
                        help='Number of expert roll outs')
    args = parser.parse_args()

    with tf.Session() as sess:

        bc_model = BCAgent(args.env_name, sess,
                           learning_rate=args.learning_rate,
                           epoch_num=args.num_epochs,
                           rollout_num=args.num_rollouts,
                           rollout_steps=args.max_timesteps)
        bc_model.train_agent()

        da_model = DAAgent(args.env_name, sess,
                           learning_rate=args.learning_rate,
                           epoch_num=args.num_epochs,
                           rollout_num=args.num_rollouts,
                           rollout_steps=args.max_timesteps)
        da_model.train_agent()

        save_stats_plot(args.env_name, bc_model, da_model)

if __name__ == '__main__':
    main()
